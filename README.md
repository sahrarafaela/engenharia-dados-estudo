# üìò Estudo de Engenharia de Dados ‚Äî Procedures, Azure, Databricks e PySpark

Este reposit√≥rio documenta os principais t√≥picos que estudei nos √∫ltimos dias como parte da minha jornada em Engenharia de Dados. Aqui voc√™ encontrar√° exemplos pr√°ticos, anota√ß√µes t√©cnicas e reflex√µes sobre:

- Procedures SQL
- Servi√ßos de dados na Azure
- Databricks
- PySpark

---

## üß© Sum√°rio

1. [Procedures SQL](#1-procedures-sql)
2. [Azure Data Services](#2-azure-data-services)
3. [Databricks](#3-databricks)
4. [PySpark](#4-pyspark)
5. [Conclus√µes e pr√≥ximos passos](#5-conclus√µes-e-pr√≥ximos-passos)

---

## 1. Procedures SQL

Durante o estudo de Procedures SQL, desenvolvi uma rotina para automatizar o processo de inclus√£o de novos registros de aluguel na base de dados insight_places. A procedure novoAluguel_44 realiza diversas valida√ß√µes e c√°lculos antes de inserir os dados, garantindo integridade e consist√™ncia.

[Procedures SQL](procedures_sql/novoAluguel_44.sql)

---

## 2. Azure Data Services

Utilizei o ambiente de notebooks do Azure Databricks para construir um pipeline de ETL que extrai dados de c√¢mbio via API, transforma com PySpark e salva em camadas de Data Lake. Tamb√©m automatizei o processo com Airflow e integrei com Slack para envio de relat√≥rios.

[Cluster Databricks](azure/databricks_cluster.md)

---

## 3. Databricks

Implementei um pipeline de ETL no ambiente Databricks utilizando PySpark. O fluxo realiza:

- Extra√ß√£o de dados de c√¢mbio via API externa
- Transforma√ß√µes com PySpark, incluindo agrega√ß√µes e convers√µes de taxa
- Salvamento dos dados em camadas `bronze` e `prata` no Data Lake (`dbfs:/`)
- Gera√ß√£o de arquivos CSV e gr√°ficos
- Envio autom√°tico de relat√≥rios e imagens para o Slack
- Automa√ß√£o do processo com Airflow, agendando notebooks diariamente

üìÅ Arquivos relacionados:

- [ETL de Extra√ß√£o](databricks/etl_extracao.py)
- [Transforma√ß√µes PySpark](databricks/transformacoes.py)
- [Envio para Slack](databricks/slack_envio.py)
- [DAG do Airflow](databricks/airflow_dag.py)

---
